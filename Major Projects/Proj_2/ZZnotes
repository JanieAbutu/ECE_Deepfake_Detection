âœ… EfficientNet-B7 Feature Extractor Structure

Your model uses:

model.backbone.features


This contains 8 blocks (0â€“7), each made up of MBConv layers:

Each MBConv block contains:

Conv2d (1Ã—1 expansion conv)

BatchNorm2d

SiLU / Swish activation

Depthwise Conv2d

BatchNorm2d

Squeeze-and-Excitation (SE) module

SE reduces channels

Fully connected layers inside SE

Conv2d (1Ã—1 projection conv)

BatchNorm2d

Skip connection (optional)

So every block includes:

Multiple Conv2d layers

Multiple BatchNorm2d layers

SE layers (which include Linear layers)

Depthwise convs

Activations (SiLU)

These are what freezing/unfreezing affects.

âœ… What EXACTLY you unfreeze in your schedule
Phase 1: Epoch 0â€“4

ðŸš« Everything in backbone.features is frozen.

Meaning these are NOT trainable:

All Conv2d weights in all 8 blocks

All BatchNorm parameters

All DepthwiseConv weights

All Squeeze-and-Excitation (SE) Linear layers

All projection/expansion conv weights

All biases (if any)

Only this is trainable:

Classifier head (Linear(in_features, 1))

Phase 2: Epoch 5â€“14 â€” Unfreeze Block 6 & Block 7

Here, you unfreeze everything inside:

ðŸ”“ Block 6 layers (trainable)

Expansion 1Ã—1 Conv2d

BatchNorm2d

SiLU

Depthwise Conv2d

BatchNorm2d

SE (Linear layers + activations)

Projection 1Ã—1 Conv2d

BatchNorm2d

Residual connection (if present)

ðŸ”“ Block 7 layers (trainable)

Same structure as block 6 â€” all of its internal Conv/BN/SE layers become trainable.

Still frozen

Blocks 0â€“5, all weights inside them, including:

Conv2d

BatchNorm2d

Depthwise

SE

Projection convs

So you only train the final ~40% of the network.

Phase 3: Epoch â‰¥ 15 â€” Unfreeze ALL Backbone Blocks

Now all blocks become trainable:

ðŸ”“ Blocks 0â€“7

Every single module becomes trainable:

Stem convolution

All MBConv blocks (0â€“7)

All Conv2d layers

All Depthwise convs

All BatchNorm2d

All Squeeze-and-Excitation (SE) Linear layers

Projection convs

This is full fine-tuning of EfficientNet-B7.

ðŸ“Œ ONE-SENTENCE SUMMARY


âœ… What is it used for? (super simple explanation)
GradScaler = prevents gradients from becoming too small in mixed precision.

When using AMP (Automatic Mixed Precision):

Your model uses float16 instead of float32

float16 is faster

but float16 has tiny precision, so gradients can underflow (become zero)

ðŸš€ GradScaler multiplies the loss by a big number â†’ so gradients donâ€™t vanish.
ðŸš€ After backprop, it shrinks them back to the correct scale.

This keeps training stable + fast.

ðŸ§  Why enabled=(DEVICE == "cuda")?

AMP works only on GPUs.

If you're on CPU, AMP is disabled automatically.
You are progressively unfreezing EfficientNet-B7â€™s MBConv blocks, which contain expansion Conv2d â†’ BatchNorm â†’ SiLU â†’ depthwise Conv2d â†’ BatchNorm â†’ SE â†’ projection Conv2d â†’ BatchNorm; blocks 6â€“7 are unfrozen first, then all blocks.